---
title: Big Data - Lecture 3, 4, & MapReduce Paper Notes
subtitle: 
author:
  - Adarsh Srivastava
date: 21 April, 2024
tags:
  - bigdata
  - nus
---
## Map Reduce

- Map/Reduce comes from the map/reduce in lisp:
	- map(f, [a,b,c,...]) = [f(a),f(b),f(c),...] (basically lambda)
	- reduce(f, [a,b,c,...]) =  f(a, f(b, f(c,...)))  [or f(f(f(a,b), c), ...)] (e.g. f can be +, \*, append, etc.)

- Map Reduce in Google terms is something similar:
	- map(k1, v1) -> list(k2, v2) 
		- These intermediate key value pairs are from a different domain than k1, v1
	- MapReduce library groups all values for a particular key into a list and passes to reduce
	- reduce(k2, list(v2)) -> list(v2) (list(v2) passed to reduce is actually an iterator to the list to handle lists too large to fit in memory)
		- Reduce uses the same intermediate k2,v2 generated by map, and they are from the same domain as output values
	- There's an optional **Combiner** that combines the values at mapper worker itself - it uses the same function as reduce, just that the combination is partial and pertaining only to that particular mapper and the output is a temp file to be read over network by the reducer workers. **But combiner DOES NOT ALWAYS RUN - hadoop runs combiner EVERY TIME there's a BUFFER SPILL to avoid network congestion. So it can be run ZERO, ONCE OR N NUMBER OF TIMES. Hence, combiner operation must be commutative and associative.** Plus, combiner output must always be the **SAME TYPE** as the mapper output and reducer input.


- In Hadoop, mapper and reducer can have setup and cleanup/close methods that are called per mapper/reducer task (as opposed to map which is called per key-value pair). You can use that in mapper to do some useful reduction before emitting.
- Working of map reduce on machines:
![Working|center|700](attachments/Screenshot%202024-04-23%20at%204.37.00%20PM.png)


- So, whole thing looks like this:
	- split input to M pieces 
	- -> spin up M mapper tasks to generate M local temp files 
	- -> (optional) use a combiner  to reduce those outputs locally and regenerate the temp files 
	- -> use the partitioning function to partition the outputs from ALL of these M temp files into R temp files on local disk on mapper node to be read by reducers. (This partition function can be overwritten) (*Since the partition function is based on keys, one key can only be on one partition, but multiple keys can be on the same partition*) (*shuffling phase on mapper machine*)
	- -> reducer reads its own partition through a network call - **It does not duplicate it on its own machine** - this is called copy phase
	- -> it sorts the intermediate data read by keys (because partitioning does not necessarily do that unless inputs were ordered) (*shuffling phase on reducer machine*)
	- -> for each key in its partition, reducer passes the key and the values (iterable) to the reduce function
	- -> whose generated output is appended to global files - one per reducer

- Explanation:
	- The MapReduce library in the user program first splits the input files into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.
	- One of the copies of the program is special – the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.
	- A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined Map function. The intermediate key/value pairs produced by the Map function are buffered in memory.
	- Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers.
	- When a reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the **local disks of the map workers**. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task.
	- The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition.
	- After successful completion, the output of the mapreduce execution is available in the R output files (one per reduce task, with file names as specified by the user). 
	- Typically, users do not need to combine these R output files into one file – they often pass these files as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple files.


- Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global file system

- The users of MapReduce specify the number of reduce tasks/output files that they desire (R). Data gets partitioned across these tasks using a partitioning function on the intermediate key. A default partitioning function is provided that uses hashing (e.g. “hash(key) mod R”). (**So one key lies completely on one partition**) This tends to result in fairly well-balanced partitions.
	- The user of the MapReduce library can provide a special partitioning function. For example, using “hash(Hostname(urlkey)) mod R” as the partitioning function causes all URLs from the same host to end up in the same output file.

- We guarantee that within a given partition, the intermediate key/value pairs are processed in increasing key order. This ordering guarantee makes it easy to generate a sorted output file per partition, which is useful when the output file format needs to support efficient random access lookups by key, or users of the output find it convenient to have the data sorted.

- In practice, we tend to choose M so that each individual task is roughly 16 MB to 64 MB of input data (so that the locality optimization described above is most effective), and we make R a small multiple of the number of worker machines we expect to use. We often perform MapReduce computations with M = 200, 000 and R = 5, 000, using 2,000 worker machines.



## HDFS (Hadoop Distributed File System)

- Assumptions:
	- horizontal scaling


## MapReduce scalability analysis

- Amdahl's law is a formula that gives the theoretical speedup for executing at fixed workload on a system whose resources could be improved. $$ S_{\text {latency }}(s)=\frac{1}{(1-p)+\frac{p}{s}} $$
	- S is the speedup of a process
	- p is the proportion of a program that can be made parallel, and
	- s is the speedup factor of the parallel portion. (or number of workers)
- For linear scalability, entire process must be parallelizable => p=1, S=s

- In Map reduce, scalability goes like this, theoretically:
	- Max # of map tasks: when each map tasks handles the smallest possible portion of input (a chunk - say 128MB) = input size/chunk size
	- Max # of reduce tasks: when each reducer handles only one key = number of distinct keys

- I/O analysis:
	- reading input = disk io
	- shuffle and sort (partition, store, and sort): network and disk io
	- output from reduce: disk and network io

- So, for word count: 
	- Mapper:
		- input io: reading the chunk = 128mb
		- intermediate io: none
		- output disk io: # of words **in the chunk**
		- network: none
	- Shuffle/Partition and sort:
		- network: # of words
		- disk: # of words
	- Reducer:
		- input disk io: network read accounted in shuffle phase 
		- intermediate: none
		- output disk io: since output is significantly *reduced*, probably not much
		- network io: writing this output, so not much

- Memory analysis
	- Since at any time the amount of data held in memory is limited to the key value pair being dealt with, the memory requirements are not very high in map reduce (remember list is iterable)
